{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9553553,"sourceType":"datasetVersion","datasetId":12713},{"sourceId":9645279,"sourceType":"datasetVersion","datasetId":5890424}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/sports-bets-winning-algorithm?scriptVersionId=226563445\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Sports Bets Winning Algorithm\n\nAuthors: [Marcus](https://www.linkedin.com/in/marcus-szczerbacki-220b0760/), [Corey](https://www.linkedin.com/in/coreyslocum/), [Cole](https://www.linkedin.com/in/william-c-wright-1614bb279/), [Don](https://www.linkedin.com/in/dontadaya/)\n\nA [DaScient Propreitary Analytic Intelligence Artifact](https://www.dascient.com).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"\"\"\"############################################################\n#               MODEL & DATA QUALITY MONITORING            #\n#           CONTINUOUS MONITORING & ALERTING GUIDE         #\n############################################################\n\nINTRODUCTION\n============\nThis document details a comprehensive strategy for continuous monitoring \nand alerting of model performance and data quality in a production \nenvironment. The goal is to ensure your predictive system remains \nrobust, reliable, and performant under all conditions, detecting issues \nlike model drift, data anomalies, and system errors in real time.\n\nARCHITECTURE OVERVIEW\n=====================\n1. Monitoring Infrastructure:\n   - Metrics Collection: Use Prometheus to collect application, system,\n     and custom metrics.\n   - Visualization: Deploy Grafana dashboards for real-time visualization \n     of performance and data quality metrics.\n   - Log Aggregation: Utilize the ELK stack (Elasticsearch, Logstash, Kibana) \n     or similar platforms (DataDog, Splunk) for centralized log analysis.\n   - Distributed Tracing: Integrate Jaeger or Zipkin to track service latency \n     and dependencies.\n\n2. Data Quality Monitoring:\n   - Implement automated data validation using tools like Great Expectations \n     or Deequ.\n   - Track key data metrics: means, medians, standard deviations, missing \n     value ratios, and distribution shifts.\n   - Perform schema validation to ensure data integrity at every ingestion point.\n\n3. Model Performance Monitoring:\n   - Continuously compute key performance indicators (KPIs) such as:\n       • Accuracy, Precision, Recall, F1 Score, ROC-AUC.\n       • Confusion Matrix trends.\n   - Monitor prediction latency, throughput, and error rates.\n   - Detect model drift using statistical tests (e.g., PSI, KL divergence) \n     and drift detection frameworks (e.g., Evidently AI).\n\n4. Alerting Mechanisms:\n   - Define threshold-based alerts for both model performance and data quality:\n       • Trigger alerts on significant drops in model accuracy or surges in error rates.\n       • Alert on data anomalies like sudden increases in missing values or \n         unexpected shifts in feature distributions.\n   - Integrate alerting with systems like PagerDuty, OpsGenie, or Slack \n     for immediate notifications.\n   - Use Prometheus Alertmanager and Grafana alerting for seamless integration.\n\nDETAILED IMPLEMENTATION STEPS\n=============================\n\nStep 1: Set Up the Monitoring Infrastructure\n--------------------------------------------\n- Install and configure Prometheus to scrape metrics from your application.\n- Deploy Grafana and create dashboards for:\n    • System metrics (CPU, memory, network).\n    • Custom model metrics (accuracy, prediction latency, error rates).\n    • Data quality metrics (feature distribution, missing values).\n- Configure a centralized logging system (ELK stack or similar) to collect \n  logs from all components.\n\nStep 2: Instrument the Model and Data Pipelines\n------------------------------------------------\n- Add instrumentation to your production code using a Prometheus client library \n  (e.g., for Python, use `prometheus_client`).\n- Expose endpoints for model metrics (e.g., /metrics) to be scraped by Prometheus.\n- Integrate detailed logging (e.g., using Loguru or Python’s logging module) \n  to capture key events, errors, and data statistics.\n- Implement data quality checks using Great Expectations:\n    • Define expectations for data schema, value ranges, and distribution.\n    • Automate periodic validation runs on incoming data batches.\n\nStep 3: Establish Data Quality Checks and Alerts\n--------------------------------------------------\n- Develop automated tests for data quality using Great Expectations or Deequ.\n- Monitor key data metrics such as:\n    • Mean, median, standard deviation per feature.\n    • Missing value ratios and outlier detection.\n- Set up alerts in Prometheus/Grafana for:\n    • Sudden deviations in feature distributions.\n    • Schema violations or high missing value rates.\n- Schedule these tests to run at regular intervals (e.g., via cron or Airflow DAGs).\n\nStep 4: Monitor Model Performance Continuously\n------------------------------------------------\n- Periodically calculate performance metrics (accuracy, ROC-AUC, etc.) on \n  incoming data or shadow deployments.\n- Use statistical tests to compare current performance against historical baselines.\n- Implement drift detection:\n    • Compute the Population Stability Index (PSI) on feature distributions.\n    • Set thresholds to trigger alerts if PSI exceeds a set limit.\n- Maintain a holdout validation set for regular performance benchmarking.\n\nStep 5: Configure Alerting Systems\n----------------------------------\n- Define alert rules in Prometheus Alertmanager:\n    • Example: Alert if model accuracy drops below a threshold (e.g., 80%).\n    • Example: Alert if the rate of 500 errors exceeds a certain level.\n- Integrate Alertmanager with your communication channels (Slack, PagerDuty, Email).\n- Configure Grafana alerts to notify on abnormal trends in dashboards.\n- Test alerts by simulating failure scenarios to verify notifications.\n\nStep 6: Automate and Orchestrate Monitoring and Retraining\n-----------------------------------------------------------\n- Use orchestration tools such as Apache Airflow or Kubeflow Pipelines to:\n    • Schedule data quality checks.\n    • Trigger model retraining when drift is detected.\n    • Update dashboards and alerting rules automatically.\n- Containerize monitoring components using Docker and manage with Kubernetes.\n- Use CI/CD pipelines to deploy updated monitoring configurations and models.\n\nStep 7: Documentation and Incident Response\n---------------------------------------------\n- Maintain comprehensive documentation (this file, runbooks, and dashboards) \n  for all monitoring and alerting configurations.\n- Create an incident response playbook:\n    • Define roles and responsibilities.\n    • Outline steps for troubleshooting and recovery.\n    • Include escalation procedures and contact information.\n- Regularly review incident logs and adjust monitoring thresholds as necessary.\n\nStep 8: Continuous Improvement and Feedback Loop\n--------------------------------------------------\n- Periodically review monitoring dashboards and alert logs.\n- Solicit feedback from operations and data science teams to refine metrics.\n- Update model and data quality monitoring strategies based on new insights.\n- Invest in automated anomaly detection and ML-based monitoring systems.\n\nDEPLOYMENT & SCALABILITY CONSIDERATIONS\n========================================\n- Use Docker and Kubernetes for scalable deployment of monitoring services.\n- Leverage Helm charts to manage Prometheus, Grafana, and Alertmanager deployments.\n- Ensure secure endpoints with TLS encryption and proper authentication.\n- Regularly backup monitoring configurations and logs.\n\nSECURITY & COMPLIANCE\n=====================\n- Secure all monitoring endpoints with access controls.\n- Encrypt sensitive data both at rest and in transit.\n- Ensure compliance with data protection regulations (GDPR, HIPAA, etc.).\n- Audit and log access to monitoring systems.\n\nCONCLUSION\n==========\nBy following these steps, you can build a robust, continuously monitored, and \nautomated system for tracking model performance and data quality. This approach \nwill help detect issues early, maintain high service quality, and facilitate \nrapid response to any operational anomalies.\n\nFor further improvements, consider integrating advanced analytics and \nmachine-learning-driven anomaly detection on your monitoring data.\n\n############################################################\n#                    END OF INSTRUCTIONS                 #\n############################################################\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Syntax","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nEnhanced Continuous Monitoring & Alerting Script\n-------------------------------------------------\n\nThis script sets up:\n  - A Flask API with two endpoints:\n      /predict : Simulated model prediction endpoint.\n      /metrics : Exposes Prometheus metrics.\n  - Background jobs that:\n      • Evaluate model performance on a holdout set.\n      • Check data quality using simulated logic (replace with real tests if needed).\n  - Prometheus metrics collection using the prometheus_client.\n  - Robust logging and error handling.\n  - Automated scheduling via APScheduler.\n\nPrerequisites:\n--------------\npip install flask prometheus_client apscheduler\n\nUsage:\n------\npython monitor.py\n\"\"\"\n\nimport logging\nimport random\nimport time\nfrom flask import Flask, Response, jsonify\nfrom prometheus_client import start_http_server, Gauge, Counter, generate_latest, CONTENT_TYPE_LATEST\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\n# Set up logging for detailed debug output.\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n\n# Define Prometheus metrics\nMODEL_ACCURACY = Gauge('model_accuracy', 'Accuracy of the predictive model')\nMODEL_LATENCY = Gauge('model_latency_ms', 'Prediction latency in milliseconds')\nDATA_QUALITY_ISSUES = Gauge('data_quality_issues', 'Number of data quality issues detected')\nTOTAL_PREDICTIONS = Counter('total_predictions', 'Total number of predictions made')\n\n# Initialize Flask app\napp = Flask(__name__)\n\n@app.route('/metrics')\ndef metrics():\n    \"\"\"Expose Prometheus metrics.\"\"\"\n    return Response(generate_latest(), mimetype=CONTENT_TYPE_LATEST)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    \"\"\"\n    Dummy prediction endpoint.\n    Simulates prediction latency and returns a random binary prediction.\n    \"\"\"\n    start_time = time.time()\n    # Simulate prediction latency between 50 and 150 milliseconds.\n    simulated_latency = random.uniform(0.05, 0.15)\n    time.sleep(simulated_latency)\n    prediction = random.choice([0, 1])\n    TOTAL_PREDICTIONS.inc()\n    elapsed_ms = (time.time() - start_time) * 1000\n    MODEL_LATENCY.set(elapsed_ms)\n    logging.info(f\"Prediction made with latency {elapsed_ms:.2f} ms, prediction: {prediction}\")\n    return jsonify({'prediction': prediction})\n\ndef run_model_evaluation():\n    \"\"\"\n    Simulated function to evaluate model performance on a holdout set.\n    Replace this simulation with your actual model evaluation logic.\n    \"\"\"\n    try:\n        # Simulate model evaluation with a random accuracy between 80% and 95%\n        simulated_accuracy = random.uniform(0.80, 0.95)\n        MODEL_ACCURACY.set(simulated_accuracy)\n        logging.info(f\"Model evaluation complete: Accuracy = {simulated_accuracy:.4f}\")\n    except Exception as e:\n        logging.error(f\"Error during model evaluation: {e}\")\n\ndef run_data_quality_check():\n    \"\"\"\n    Simulated function to check data quality.\n    Replace the simulation with real data quality tests (e.g., using Great Expectations).\n    \"\"\"\n    try:\n        # Simulate data quality check: random number of issues (0 to 5)\n        simulated_issues = random.randint(0, 5)\n        DATA_QUALITY_ISSUES.set(simulated_issues)\n        if simulated_issues > 0:\n            logging.warning(f\"Data quality check detected {simulated_issues} issues.\")\n        else:\n            logging.info(\"Data quality check passed with no issues.\")\n    except Exception as e:\n        logging.error(f\"Error during data quality check: {e}\")\n\ndef start_scheduler():\n    \"\"\"\n    Start background scheduler to periodically run model evaluation and data quality checks.\n    \"\"\"\n    scheduler = BackgroundScheduler()\n    scheduler.add_job(run_model_evaluation, 'interval', seconds=60, id='model_evaluation_job')\n    scheduler.add_job(run_data_quality_check, 'interval', seconds=60, id='data_quality_job')\n    scheduler.start()\n    logging.info(\"Scheduler started with model evaluation and data quality check jobs.\")\n\ndef main():\n    # Start the Prometheus HTTP metrics server on port 8000.\n    start_http_server(8000)\n    logging.info(\"Prometheus metrics server started on port 8000.\")\n    \n    # Start background jobs for continuous monitoring.\n    start_scheduler()\n    \n    # Run the Flask application to serve prediction endpoint and metrics.\n    app.run(host='0.0.0.0', port=5000)\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Project by Marcus, Corey, Cole, & Don.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:21:00.040941Z","iopub.execute_input":"2024-10-17T17:21:00.041403Z","iopub.status.idle":"2024-10-17T17:21:00.06788Z","shell.execute_reply.started":"2024-10-17T17:21:00.041352Z","shell.execute_reply":"2024-10-17T17:21:00.066259Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/collegefootballpoll-com/weekly-picks-and-scores_fbs_101524-3.pdf\n/kaggle/input/nfl-scores-and-betting-data/nfl_stadiums.csv\n/kaggle/input/nfl-scores-and-betting-data/nfl_teams.csv\n/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv\n/kaggle/input/nfl-scores-and-betting-data/spreadspoke.R\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from IPython.display import clear_output\n\n!pip install tabula\n!pip install pdfminer\nclear_output()\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport pdfminer as pdf2txt\nimport io\nfrom pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nfrom io import BytesIO","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:22:01.720937Z","iopub.execute_input":"2024-10-17T17:22:01.721425Z","iopub.status.idle":"2024-10-17T17:23:00.785511Z","shell.execute_reply.started":"2024-10-17T17:22:01.721393Z","shell.execute_reply":"2024-10-17T17:23:00.783882Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import tabula \ncsv = \"/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv\"\npdf = \"/kaggle/input/collegefootballpoll-com/weekly-picks-and-scores_fbs_101524-3.pdf\"\n\n#output = tabula.convert_into(file, \"converted.csv\", output_format=\"csv\", lattice=True, stream=False,  pages=\"all\" )","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:24:49.875834Z","iopub.execute_input":"2024-10-17T17:24:49.876309Z","iopub.status.idle":"2024-10-17T17:24:49.883Z","shell.execute_reply.started":"2024-10-17T17:24:49.876275Z","shell.execute_reply":"2024-10-17T17:24:49.88119Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nEnhanced Sports Data Pipeline and Predictive Modeling Script\n=============================================================\n\nThis script performs the following operations:\n  1. Ingests NFL scores and betting data from a CSV file.\n  2. Ingests College Football weekly picks and scores from a PDF.\n  3. Cleans and augments both datasets with feature engineering.\n     - For NFL data, it creates outcome and margin features.\n     - For College Football data, it renames duplicate SCORE columns,\n       converts dates/scores, and creates an outcome (favorite win) feature.\n  4. Performs exploratory data analysis (EDA) with summary statistics and plots.\n  5. Trains a basic predictive model (logistic regression) on NFL data.\n  6. Logs detailed progress and errors for debugging and monitoring.\n\nData Sources:\n  - NFL CSV: \"/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv\"\n  - College Football PDF: \"/kaggle/input/collegefootballpoll-com/weekly-picks-and-scores_fbs_101524-3.pdf\"\n    (Expected columns: DATE, SCORE, FAVORITE, LINE, COMP, UNDERDOG, SCORE)\n\nPrerequisites:\n  pip install pandas numpy matplotlib seaborn scikit-learn tabula-py\n\nUsage:\n  python enhanced_pipeline.py\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport logging\nimport pickle\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For PDF extraction (ensure Java is installed for tabula)\nimport tabula\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Configure logging for detailed debug output.\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n\n# Define file paths (adjust if needed)\nNFL_CSV_PATH = \"/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv\"\nCF_PDF_PATH = \"/kaggle/input/collegefootballpoll-com/weekly-picks-and-scores_fbs_101524-3.pdf\"\n\ndef process_nfl_csv(csv_path):\n    \"\"\"\n    Process NFL scores and betting data from CSV.\n    Returns a cleaned pandas DataFrame.\n    \"\"\"\n    try:\n        df = pd.read_csv(csv_path)\n        logging.info(f\"NFL CSV loaded with shape: {df.shape}\")\n        \n        # Basic cleaning: remove rows with missing values.\n        df.dropna(inplace=True)\n        \n        # If a date column exists, parse it; adjust column name as needed.\n        if 'date' in df.columns:\n            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n        else:\n            logging.warning(\"No 'date' column found in NFL CSV.\")\n        \n        # Create outcome and margin features if score columns exist.\n        # We assume the CSV contains 'home_score' and 'away_score'. If not, adjust accordingly.\n        if 'home_score' in df.columns and 'away_score' in df.columns:\n            df['home_win'] = (df['home_score'] > df['away_score']).astype(int)\n            df['margin'] = df['home_score'] - df['away_score']\n            logging.info(\"Outcome and margin features created for NFL data.\")\n        else:\n            logging.warning(\"Score columns not found in NFL CSV; skipping outcome creation.\")\n        \n        return df\n    except Exception as e:\n        logging.error(f\"Error processing NFL CSV: {e}\")\n        sys.exit(1)\n\ndef process_cf_pdf(pdf_path):\n    \"\"\"\n    Process College Football picks and scores from a PDF.\n    Returns a cleaned pandas DataFrame.\n    \n    Expected PDF columns (in order):\n      DATE, SCORE, FAVORITE, LINE, COMP, UNDERDOG, SCORE\n    We'll rename the duplicate SCORE columns to 'favorite_score' and 'underdog_score'.\n    \"\"\"\n    try:\n        # Extract all tables from the PDF (assumes the PDF is tabular in nature)\n        dfs = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n        if not dfs:\n            logging.error(\"No tables found in College Football PDF.\")\n            sys.exit(1)\n        # Concatenate tables if multiple are found.\n        df = pd.concat(dfs, ignore_index=True)\n        logging.info(f\"College Football PDF loaded with shape: {df.shape}\")\n        \n        # Check the number of columns and rename appropriately.\n        if len(df.columns) >= 7:\n            # Rename columns assuming the order:\n            # DATE, favorite_score, FAVORITE, LINE, COMP, UNDERDOG, underdog_score\n            df.columns = ['DATE', 'favorite_score', 'FAVORITE', 'LINE', 'COMP', 'UNDERDOG', 'underdog_score']\n            logging.info(\"PDF columns renamed to standardized names.\")\n        else:\n            logging.warning(\"Unexpected number of columns in PDF table.\")\n        \n        # Convert DATE column to datetime.\n        df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n        \n        # Convert score columns to numeric.\n        df['favorite_score'] = pd.to_numeric(df['favorite_score'], errors='coerce')\n        df['underdog_score'] = pd.to_numeric(df['underdog_score'], errors='coerce')\n        \n        # Remove rows with missing crucial values.\n        df.dropna(subset=['DATE', 'favorite_score', 'underdog_score'], inplace=True)\n        \n        # Create outcome column: 1 if favorite wins, 0 otherwise.\n        df['favorite_win'] = (df['favorite_score'] > df['underdog_score']).astype(int)\n        df['margin'] = df['favorite_score'] - df['underdog_score']\n        logging.info(\"Outcome and margin features created for College Football data.\")\n        return df\n    except Exception as e:\n        logging.error(f\"Error processing College Football PDF: {e}\")\n        sys.exit(1)\n\ndef eda_nfl(df):\n    \"\"\"\n    Perform exploratory data analysis on the NFL data.\n    \"\"\"\n    logging.info(\"Performing EDA on NFL data.\")\n    print(\"NFL Data (First 5 Rows):\")\n    print(df.head())\n    print(\"\\nNFL Data Summary:\")\n    print(df.describe())\n    \n    # Plot distribution of score margins if available.\n    if 'margin' in df.columns:\n        plt.figure(figsize=(8,6))\n        sns.histplot(df['margin'], bins=20, kde=True)\n        plt.title(\"Distribution of Score Margin (NFL)\")\n        plt.xlabel(\"Margin\")\n        plt.ylabel(\"Frequency\")\n        plt.show()\n\ndef eda_cf(df):\n    \"\"\"\n    Perform exploratory data analysis on the College Football data.\n    \"\"\"\n    logging.info(\"Performing EDA on College Football data.\")\n    print(\"College Football Data (First 5 Rows):\")\n    print(df.head())\n    print(\"\\nCollege Football Data Summary:\")\n    print(df.describe())\n    \n    # Plot distribution of score margins.\n    if 'margin' in df.columns:\n        plt.figure(figsize=(8,6))\n        sns.histplot(df['margin'], bins=20, kde=True)\n        plt.title(\"Distribution of Score Margin (College Football)\")\n        plt.xlabel(\"Margin\")\n        plt.ylabel(\"Frequency\")\n        plt.show()\n\ndef train_model_nfl(df):\n    \"\"\"\n    Train a predictive model on NFL data using logistic regression.\n    Assumes that the CSV contains a target column 'home_win' and at least one numerical feature.\n    Here we attempt to use betting lines as features (e.g., column 'LINE' or 'spread').\n    If not available, a dummy feature based on margin is used.\n    \"\"\"\n    logging.info(\"Training predictive model on NFL data.\")\n    \n    # Determine feature columns based on available columns.\n    feature_cols = []\n    if 'LINE' in df.columns:\n        feature_cols.append('LINE')\n    elif 'spread' in df.columns:\n        feature_cols.append('spread')\n    \n    if not feature_cols:\n        logging.warning(\"No explicit betting feature found; using 'margin' as a dummy feature.\")\n        if 'margin' not in df.columns:\n            logging.error(\"No feature available for model training.\")\n            sys.exit(1)\n        df['dummy_feature'] = df['margin']\n        feature_cols = ['dummy_feature']\n    \n    if 'home_win' not in df.columns:\n        logging.error(\"Target column 'home_win' not found in NFL data.\")\n        sys.exit(1)\n    \n    X = df[feature_cols]\n    y = df['home_win']\n    \n    # Split data into training and testing sets.\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X_train, y_train)\n    \n    preds = model.predict(X_test)\n    acc = accuracy_score(y_test, preds)\n    logging.info(f\"Model trained. Test accuracy: {acc:.4f}\")\n    print(\"Classification Report:\")\n    print(classification_report(y_test, preds))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, preds))\n    \n    return model\n\ndef main():\n    # Process and analyze NFL CSV data.\n    logging.info(\"Starting NFL CSV processing...\")\n    nfl_df = process_nfl_csv(NFL_CSV_PATH)\n    eda_nfl(nfl_df)\n    \n    # Process and analyze College Football PDF data.\n    logging.info(\"Starting College Football PDF processing...\")\n    cf_df = process_cf_pdf(CF_PDF_PATH)\n    eda_cf(cf_df)\n    \n    # Train a predictive model on NFL data.\n    model = train_model_nfl(nfl_df)\n    \n    # Save the trained model to disk for future use.\n    model_filename = \"nfl_model.pkl\"\n    with open(model_filename, \"wb\") as f:\n        pickle.dump(model, f)\n    logging.info(f\"Predictive model saved to {model_filename}\")\n    \n    logging.info(\"Data processing and model training complete.\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install PyPDF2\n!pip install pdfplumber\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:28:51.320706Z","iopub.execute_input":"2024-10-17T17:28:51.321174Z","iopub.status.idle":"2024-10-17T17:29:21.589378Z","shell.execute_reply.started":"2024-10-17T17:28:51.321111Z","shell.execute_reply":"2024-10-17T17:29:21.587803Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from PyPDF2 import PdfReader\n\nreader = PdfReader(pdf)\nnumber_of_pages = len(reader.pages)\npage = reader.pages[0]\ntext = page.extract_text()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:29:21.592017Z","iopub.execute_input":"2024-10-17T17:29:21.592465Z","iopub.status.idle":"2024-10-17T17:29:21.649501Z","shell.execute_reply.started":"2024-10-17T17:29:21.592425Z","shell.execute_reply":"2024-10-17T17:29:21.648161Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def pdf_to_text(path):\n    manager = PDFResourceManager()\n    retstr = io.StringIO()\n    layout = LAParams(all_texts=True)\n    device = TextConverter(manager, retstr)\n    filepath = open(path, 'rb')\n    interpreter = PDFPageInterpreter(manager, device)\n    for page in PDFPage.get_pages(filepath, caching=True,check_extractable=True):\n        #print(page[0])\n        interpreter.process_page(page)\n        text = retstr.getvalue()\n    filepath.close()\n    device.close()\n    retstr.close()\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        fn=os.path.join(dirname, filename)\n        print(fn)\n        #if fn.endswith('.pdf'):\n        #   pdf2txt.append(fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stadiums = pd.read_csv(\"/kaggle/input/nfl-scores-and-betting-data/nfl_stadiums.csv\",header=0,encoding='unicode_escape')\nstadiums","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stadiums = pd.read_csv(\"/kaggle/input/nfl-scores-and-betting-data/nfl_stadiums.csv\",header=0,encoding='unicode_escape')\nteams = pd.read_csv(\"/kaggle/input/nfl-scores-and-betting-data/nfl_teams.csv\",header=0)\nscores = pd.read_csv(\"/kaggle/input/nfl-scores-and-betting-data/spreadspoke_scores.csv\",header=0)\n\n\n#stadiums['stadium_weather_station_code'] = stadiums['stadium_weather_station_code'].astype('float')\n#stadiums['stadium_capacity'] = stadiums['stadium_capacity'].astype('float')\nstadiums['LATITUDE'] = stadiums['LATITUDE'].astype('float')\nstadiums['LONGITUDE'] = stadiums['LONGITUDE'].astype('float')\nstadiums['ELEVATION'] = stadiums['ELEVATION'].astype('float')\n\n\nscores['schedule_date'] = scores['schedule_date'].astype('datetime64[ns]')\nscores['schedule_season'] = scores['schedule_season'].astype('datetime64[ns]').dt.year\nscores['weather_temperature'] = scores['weather_temperature'].astype('float')\nscores['score_home'] = scores['score_home'].astype('float')\nscores['score_away'] = scores['score_away'].astype('float')\nscores['weather_temperature'] = scores['weather_temperature'].astype('float')\nscores['weather_wind_mph'] = scores['weather_wind_mph'].astype('float')\nscores['weather_humidity'] = scores['weather_humidity'].astype('float')\nscores['schedule_season'] = scores['schedule_season'].astype('float')\n\n# Create a winners list.\nwinner =[]\n# Obtain the scores for each area\nfor i,v in scores.score_home.items():\n    if scores.score_home[i]>scores.score_away[i]:\n        winner.append(scores.team_home[i])\n    \n    elif scores.score_home[i]<scores.score_away[i]:\n        winner.append(scores.team_away[i])\n    else:\n        winner.append('Tie')\n\nscores['winner'] = winner","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# What does our data look like?","metadata":{}},{"cell_type":"code","source":"# show sample of dataset\nscores.sample(25).sort_values('schedule_date',ascending=False).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# And we have a winner!","metadata":{}},{"cell_type":"code","source":"scores[['team_home','score_home','score_away','team_away','winner']].sample(25)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Atlanta Falcons - Wins\nCorrelation map.","metadata":{}},{"cell_type":"code","source":"from string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\natlanta = scores[scores.team_home == 'Atlanta Falcons'].sort_values('schedule_date',ascending=False).reset_index(drop=True)\n\nsns.set_theme(style=\"white\")\n\n\n# Compute the correlation matrix\ncorr = atlanta.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation map of factors when Falcons win.","metadata":{}},{"cell_type":"code","source":"# When do the Falcons win?\n\natlanta_wins = atlanta[atlanta.score_home > atlanta.score_away]\n\nfrom string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"white\")\n\n\n# Compute the correlation matrix\ncorr = atlanta_wins.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"xtick.labelsize\"] = 6\n# plot a sample of 100 observations that lasted under 60 minutes\n# need to get a smaller sample of city-set, the x-axis is way too muddled.\nsns.catplot(data=atlanta_wins.sample(100), x=\"schedule_date\", \n            y=\"score_home\", \n            hue=\"team_away\", \n            kind=\"swarm\", \n            height=10, \n            aspect=2, \n            #size='score_away',\n            #size_max=20\n           )\nplt.xticks(rotation=45)\nplt.ylim(bottom=0)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nfig = px.scatter_3d(scores, z='team_home', y='score_home', x='stadium',\n              color='stadium',\n              size = 'score_home',\n              symbol = 'winner',\n              #hover_name = 'shape',\n              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],\n              opacity=0.7,\n              size_max=25\n                    \n                   )\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nfig = px.scatter_3d(atlanta_wins, z='schedule_date', y='score_home', x='stadium',\n              color='stadium',\n              size = 'score_home',\n              #symbol = 'schedule_playoff',\n              #hover_name = 'shape',\n              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],\n              opacity=0.7,\n              size_max=25\n                    \n                   )\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nfig = px.scatter_3d(scores, z='schedule_date', y='team_home', x='stadium',\n              color='team_away',\n              size = 'score_home',\n              #symbol = 'stadium',\n              hover_data=['winner','schedule_date','weather_humidity','weather_detail','score_away','team_away'],\n              opacity=0.7,\n              size_max=25\n                    \n                   )\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predictive Modeling - Who will win?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# encoding\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode(df):\n    lb_make = LabelEncoder()\n    columns = df.columns.values.tolist()\n    df_encoded = df[columns].copy()\n\n    # categorize/encode\n    for i in columns:\n        df_encoded[i] = lb_make.fit_transform(df[i])\n\n    # encoded\n    return df_encoded\n\n# create X,y variables for ML\nfrom sklearn.model_selection import train_test_split\ndef X_y_sets(df, target):\n    X = df.dropna().drop(columns=[target]).copy()\n    y = df.dropna()[target].ravel().copy()\n    \n    return train_test_split(X, y, test_size=0.33, random_state=42), X, y\n\n\n# encoded variable re-mapping\ndef encoding_remap(df, df_encoded, target):\n    \n    X_test = X_y_sets(df, target)[0][0]\n    \n    remap = pd.merge(df_encoded.loc[df_encoded.index.isin(X_test.index.values)][target].reset_index(),\n              df.loc[df.index.isin(X_test.index.values)][[target]].reset_index(),on=['index'])\n    \n    remap[target] = [str(remap[f'{target}_y'][i]) for i,v in remap[f'{target}_x'].items()]\n    remap['index'] = np.array([str(remap[f'{target}_x'][i]) for i,v in remap[f'{target}_x'].items()]).astype(int)\n    remap=remap[[target,'index']]\n    remap = remap.set_index('index').drop_duplicates().sort_values('index')\n    \n    return remap\n\n\n# pairplot\nimport seaborn as sns\ndef pairplot(df, target):\n    return sns.pairplot(df,hue=target)\n    \nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n\n# classifier iteration\ndef classification_feat_importance(df_encoded):\n    \n    # iterate through each column variable as classification targets\n    for target in df_encoded.columns.values:\n        X = df_encoded.drop(columns=[target]).copy()\n        y = df_encoded[target].ravel().copy()\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n        \n    \n        # classifiers\n        #clf1 = GradientBoostingClassifier(criterion=\"friedman_mse\", init=None, learning_rate=0.3338, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n        #clf2 = GradientBoostingClassifier(criterion=\"squared_error\", init=None, learning_rate=0.2222, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n        clf3 = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=42).fit(X_train, y_train)\n        clf4 = ExtraTreesClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n        clf5 = AdaBoostClassifier(n_estimators=8000, random_state=42).fit(X_train, y_train)\n        clf6 = MLPClassifier(alpha=1, max_iter=500).fit(X_train, y_train)\n        clf7 = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)\n        classifiers = [\n                       #clf1, \n                       #clf2, \n                       clf3, \n                       clf4, \n                       clf5,\n                       clf6,\n                       #clf7\n                      ]\n\n        for classifier in classifiers:\n            results = []\n            results.append({\"classifier\":str(classifier).split(\"(\")[0],\"target\":target,\"test_score\":classifier.score(X_test, y_test)})\n            for i in results:\n                if target == 'verified':\n                    print(\"\\nClassifier:\",str(classifier).split(\"(\")[0],\"\\nTarget:\",target,\"\\nScore:\",classifier.score(X_test, y_test))\n        \n        test_matrix = confusion_matrix(y_test, clf.predict(X_test)) \n        results = pd.DataFrame(results)\n        \n    return results,test_matrix\n\nprint(\"To analyze which target-classifier would yield the best results: \\nUncomment (#) the code below.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# is scaling necessary?\n# construction of ML dataframes\ntarget = 'winner'\n\n# copy\na = scores.copy()\n\n# for the sake of computationa efficiency\na = a","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find random sample & save index for defining an encoded use-case\nfrom random import randrange\nidx = randrange(len(a))\n\n# print random configuration item\nprint(\"\\nThis is a randomly chosen subject we will try to predict.\")\nb = pd.DataFrame(a.loc[idx]).T\nprint(f\"\\nTarget:'{target}' value is: \",b.reset_index()[target][0],\"\\n\")\n\n# store sol'n\nsolution = str(b.reset_index()[target][0])\n\n# print data point\nb\n# if this cell fails, try it again from step 1 - you ran into a null variable (i'll fix that soon enough)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# categorize/encode entire dataframe(a)\nc = encode(a)\nprint(\"\\nOriginal dataframe encoded into something we can run a classifier against.\\n\")\nc.sample(10).reset_index(drop=True).style.background_gradient(cmap ='Pastel1').set_properties(**{'font-size': '10px'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 'comments' & 'country' - out\nsns.pairplot(c.copy(),\n             hue=f'{target}',\n             kind=\"kde\",\n             corner=True,\n             palette=\"Paired\"\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print encoded item\nuse_case = pd.DataFrame(c.loc[idx]).T.drop(columns=[target]) \n\n#c\n\n# print encoded item w/out target info\ndata = c.drop(columns=[target,'score_home','score_away']) \n\nprint(\"\\nThis is what our encoded 'use-case' looks like - number form, just the way the machine likes it.\\n\")\n\nuse_case.style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Here we train the machine using previous scores, weather, stadium, & the over_under_lines variables. \n### However, we do remove the score_home & score_away variables from our use-case because we don't want the machine to be godly omniscient.","metadata":{}},{"cell_type":"code","source":"# create X,y variables for ML\n# save trainer\nprint(\"\\nResetting train data...\\nCreating X-matrix & y-vector (target) for classification.\")\ntrainer = c.loc[c.index!=idx].copy()\nX, y =  trainer.drop(columns=[target,'score_home','score_away']), trainer[target].ravel()\nX_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train['target'] = pd.Series(y_train)\nX_train.dropna().head().reset_index(drop=True).reset_index(drop=True).style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for the sake of adding the 'target' column above for sake of layman's explanation\nX_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encoded variable re-mapping\n# specific to our current target choice\nd = encoding_remap(a, c, target)\nprint(\"\\nDecoding our encoded dataframe to correlate with the initial randomly chosen subject.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n-Live prediction-\\nThinking...\\n\")\n\n# choose classifier\n#clf = GradientBoostingClassifier(criterion=\"friedman_mse\", init=None, learning_rate=0.3338, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n#clf = GradientBoostingClassifier(criterion=\"squared_error\", init=None, learning_rate=0.2222, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n\n# these ones run just a little more efficiently for now\n#clf = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=42).fit(X_train, y_train)\n#clf = ExtraTreesClassifier(n_estimators=1000, random_state=42).fit(X_train, y_train)\n#clf = AdaBoostClassifier(n_estimators=1500, random_state=42).fit(X_train, y_train)\nclf = MLPClassifier(alpha=0.666, max_iter=666).fit(X_train, y_train)\n#clf = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)\n\nprint()\nprint(\"Test score (confidence): \",clf.score(X_test, y_test)*100,\"%\")\nprint()\nprediction = clf.predict(use_case)[0]\nprint(f\"Prediction {target} index:\",prediction)\n\n# print decoded prediction\nprint(\"\\nPrediction Decoded\")\ne = d[d.index == prediction]\ne","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Was our prediction model successful?","metadata":{}},{"cell_type":"code","source":"solved = str(e.winner[e.index[0]])\nif solution == solved:\n    print(f\"\\nYUP!\\n\\nThe machine correctly predicted the '{target}'!\\n\")\nelse:\n    print(\"\\nNOPE!\\nThe machine's prediction was incorrect :(\")\n    \nprint()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Done for now!","metadata":{}}]}